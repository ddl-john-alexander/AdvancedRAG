{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.chains import ConversationChain, HypotheticalDocumentEmbedder, LLMChain\n",
    "from domino_data.vectordb import DominoPineconeConfiguration\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain import PromptTemplate\n",
    "from langchain_community.chat_models import ChatMlflow\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever, CohereRagRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "import sys\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "client = get_deploy_client(os.environ['DOMINO_MLFLOW_DEPLOYMENTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf691125fc540489e239bdd0e6a5caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32ba1c73ac547fab00ba508e944b900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cdfa140b494a8b9c9370e2e2b8f2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f56ffe476348a3987688a4537adecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e27219b1f4455aa556d700d9a14b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc602a029fbd41dab5f60cf537a902fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625364f20c434145b48d3075d26fc567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c5a3027fa645a1a4942b43a8bbcd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533c431671c448789a50ccb4b2905c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02812251d6b24eb3b257c7fc9b66637d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34861edcd11435fb8eb2b82abded9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model_name = \"BAAI/bge-small-en\"\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = './model_cache/'\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=embedding_model_name,\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PINECONE_ENV=os.getenv('PINECONE_ENV')\n",
    "COHERE_API_KEY=os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Domino Pinecone Data Source name \n",
    "datasource_name = \"mrag-fin-docs-ja\"\n",
    "\n",
    "# Load Domino Pinecone Data Source Configuration \n",
    "conf = DominoPineconeConfiguration(datasource=datasource_name)\n",
    "\n",
    "# api_key variable is a mandatory non-empty field used for \n",
    "# the native pinecone python client initialization \n",
    "# using Pinecone Data Source name \n",
    "api_key = os.environ.get(\"DOMINO_VECTOR_DB_METADATA\", datasource_name)\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=api_key, \n",
    "    environment=PINECONE_ENV,\n",
    "    openapi_config=conf # Domino Pinecone Data Source Configuration \n",
    ")\n",
    "\n",
    "# Load Pinecone Index\n",
    "index_name = \"mrag-fin-docs\"\n",
    "index = pinecone.Index(index_name)\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "vectorstore = Pinecone(\n",
    "    index, embeddings.embed_query, text_field # Using embedded data from Domino AI Gateway Endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.00419,\n",
       " 'namespaces': {'': {'vector_count': 419}},\n",
       " 'total_vector_count': 419}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatLLM = ChatMlflow(\n",
    "        target_uri=os.environ[\"DOMINO_MLFLOW_DEPLOYMENTS\"],\n",
    "        endpoint=\"chat-gpt4-ja\", \n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "conversationChat = ConversationChain(\n",
    "    llm=chatLLM,\n",
    "    memory=ConversationSummaryMemory(llm=chatLLM),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"How can I help you today?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Cohere's reranker with the vector DB using Cohere's embeddings as the base retriever\n",
    "cohere_rerank = CohereRerank(cohere_api_key=\"{API_KEY}\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=cohere_rerank, \n",
    "    base_retriever=db.as_retriever()\n",
    ")\n",
    "compressed_docs = compression_retriever.get_relevant_documents(user_query)\n",
    "# Print the relevant documents from using the embeddings and reranker\n",
    "print(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup HyDE\n",
    "\n",
    "hyde_prompt_template = \"\"\"As an advanced Retrieve-and-Generate (RAG) Chatbot with expertise in financial analysis, your task is to dissect corporate filings (e.g., 10-K, 10-Q, 8-K reports) of publicly traded companies and provide detailed, accurate responses to the following user question about the company's financial health, market position, and future prospects. When interacting, adhere to the following guidelines:\n",
    "Your primary objective is to deliver insightful, accurate, and helpful answers that enable users to make informed decisions based on corporate filings analysis. Each response should be tailored to the user's question, enhancing understanding of the company's financial status and strategic direction. \n",
    "\"Please answer the user's question below \\n \n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "hyde_prompt = PromptTemplate(input_variables=[\"question\"], template=hyde_prompt_template)\n",
    "hyde_llm_chain = LLMChain(llm=chatLLM, prompt=hyde_prompt)\n",
    "\n",
    "hyde_embeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=hyde_llm_chain, base_embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get relevant docs through vector DB\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Number of texts to match (may be less if no suitable match)\n",
    "NUM_TEXT_MATCHES = 5\n",
    "\n",
    "# Number of texts to return from reranking\n",
    "NUM_RERANKING_MATCHES = 3\n",
    "\n",
    "# Create prompt\n",
    "template = \"\"\"As an advanced Retrieve-and-Generate (RAG) Chatbot with expertise in financial analysis, your task is to dissect corporate filings (e.g., 10-K, 10-Q, 8-K reports) of publicly traded companies and provide detailed, accurate responses to the following user question about the company's financial health, market position, and future prospects. When interacting, adhere to the following guidelines:If the question is not related to the context of corporate filings, politely respond with, 'Hi, I'm here to help analyze financial documents and related inquiries. Could you ask a question related to the company's corporate filings?'\n",
    "Your primary objective is to deliver insightful, accurate, and helpful answers that enable users to make informed decisions based on corporate filings analysis. Each response should be tailored to the user's question, enhancing understanding of the company's financial status and strategic direction. pertaining to policy coverage.\n",
    "Here is some relevant context: {context}\n",
    "\"\"\"\n",
    "\n",
    "# Load the reranking model\n",
    "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Get relevant docs through vector DB\n",
    "def get_relevant_docs(user_input, num_matches=NUM_TEXT_MATCHES, use_hyde=False):\n",
    "   \n",
    "    if use_hyde:\n",
    "        embedded_query = hyde_embeddings.embed_query(user_input)\n",
    "    else:\n",
    "        embedded_query = embeddings.embed_query(user_input)\n",
    "        \n",
    " \n",
    "    relevant_docs = index.query(\n",
    "        vector=embedded_query,\n",
    "        top_k=num_matches,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    matches = relevant_docs[\"matches\"]\n",
    "    filtered_matches = [match for match in matches if match['score'] >= SIMILARITY_THRESHOLD]\n",
    "    relevant_docs[\"matches\"] = filtered_matches\n",
    "\n",
    "    return relevant_docs\n",
    "\n",
    " \n",
    "def build_system_prompt(user_input, rerank=False, use_hyde=False):\n",
    "    print(user_input)\n",
    "    try:\n",
    "        relevant_docs = get_relevant_docs(user_input)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get relevant documents: {e}\")\n",
    "        return \"\", \"Failed to get relevant documents\"\n",
    "\n",
    "    actual_num_matches = len(relevant_docs[\"matches\"])\n",
    "    if actual_num_matches == 0:\n",
    "        print(\"No matches found in relevant documents.\")\n",
    "        return \"\", \"No matches found in relevant documents\"\n",
    "    \n",
    "    contexts = [relevant_docs[\"matches\"][i][\"metadata\"][\"text\"] for i in range(actual_num_matches)]\n",
    "    print(\"num_matches: \", actual_num_matches)\n",
    "    if rerank and actual_num_matches >= NUM_RERANKING_MATCHES:\n",
    "        try:\n",
    "            docs = colbert.rerank(query=user_input, documents=contexts, k=NUM_RERANKING_MATCHES)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to rerank documents: {e}\")\n",
    "            return \"\", \"Failed to rerank documents\"\n",
    "        \n",
    "        try:\n",
    "            result_indices = [docs[i][\"result_index\"] for i in range(NUM_RERANKING_MATCHES)]\n",
    "        except (IndexError, KeyError) as e:\n",
    "                print(f\"Invalid result indices: {e}\")\n",
    "                return \"\", \"Invalid result indices\"\n",
    "        try:    \n",
    "            contexts = [contexts[index] for index in result_indices]\n",
    "        except IndexError as e:\n",
    "            print(f\"Indexing error: {e}\")\n",
    "            return \"\", \"Indexing error\"\n",
    "    \n",
    "    # Create prompt\n",
    "    template = \"\"\"As an advanced Retrieve-and-Generate (RAG) Chatbot with expertise in financial analysis, your task is to dissect corporate filings (e.g., 10-K, 10-Q, 8-K reports) of publicly traded companies and provide detailed, accurate responses to the following user question about the company's financial health, market position, and future prospects. When interacting, adhere to the following guidelines\n",
    "Your primary objective is to deliver insightful, accurate, and helpful answers that enable users to make informed decisions based on corporate filings analysis. Each response should be tailored to the user's question, enhancing understanding of the company's financial status and strategic direction.\n",
    "Here is some relevant context: {context}\"\"\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    system_prompt = prompt_template.format(context=contexts)\n",
    "    print(contexts)\n",
    "    return system_prompt, contexts\n",
    "\n",
    "# Query the Open AI Model\n",
    "def queryAIModel(user_input, llm_name=\"openai\", use_hyde=False):\n",
    "\n",
    "    if not user_input:\n",
    "        return \"Please provide an input\"\n",
    "    \n",
    "    system_prompt = build_system_prompt(user_input)\n",
    "   # print(system_prompt)\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=system_prompt\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=user_input\n",
    "        ),\n",
    "    ]\n",
    "    Print(\"hi\")\n",
    "    output = conversationChat.predict(input=messages)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide your question here : What is Apple's revenue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Apple's revenue\n",
      "num_matches:  5\n",
      "['Apple Inc.\\nCONSOLIDA TED ST ATEMENTS OF OPERA TIONS\\n(In millions, except number of shares, which are reflected in thousands, and per-share amounts)\\nYears ended\\nSeptember 30,\\n2023September 24,\\n2022September 25,\\n2021\\nNet sales:\\n   Products $ 298,085 $ 316,199 $ 297,392 \\n   Services 85,200 78,129 68,425 \\nTotal net sales 383,285 394,328 365,817 \\nCost of sales:\\n   Products 189,282 201,471 192,266 \\n   Services 24,855 22,075 20,715 \\nTotal cost of sales 214,137 223,546 212,981 \\nGross margin 169,148 170,782 152,836 \\nOperating expenses:\\nResearch and development 29,915 26,251 21,914 \\nSelling, general and administrative 24,932 25,094 21,973 \\nTotal operating expenses 54,847 51,345 43,887 \\nOperating income 114,301 119,437 108,949', 'A reconciliation of the Company’ s segment operating income to the Consolidated Statements of Operations for 2023, 2022 and 2021 is as follows (in millions):\\n2023 2022 2021\\nSegment operating income $ 150,888 $ 152,895 $ 137,006 \\nResearch and development expense (29,915) (26,251) (21,914)\\nOther corporate expenses, net (6,672) (7,207) (6,143)\\nTotal operating income $ 114,301 $ 119,437 $ 108,949 \\n(1)Includes corporate marketing expenses, certain share-based compensation expenses, various nonrecurring charges, and other separately managed general\\nand administrative costs.(1)\\nApple Inc. | 2023 Form 10-K | 47', 'Apple Inc.\\nCONSOLIDA TED ST ATEMENTS OF CASH FLOWS\\n(In millions)\\nYears ended\\nSeptember 30,\\n2023September 24,\\n2022September 25,\\n2021\\nCash, cash equivalents and restricted cash, beginning balances $ 24,977 $ 35,929 $ 39,789 \\nOperating activities:\\nNet income 96,995 99,803 94,680 \\nAdjustments to reconcile net income to cash generated by operating activities:\\nDepreciation and amortization 11,519 11,104 11,284 \\nShare-based compensation expense 10,833 9,038 7,906 \\nOther (2,227) 1,006 (4,921)\\nChanges in operating assets and liabilities:\\nAccounts receivable, net (1,688) (1,823) (10,125)\\nVendor non-trade receivables 1,271 (7,520) (3,903)\\nInventories (1,618) 1,484 (2,642)\\nOther current and non-current assets (5,684) (6,499) (8,042)', 'Apple Inc.\\nCONSOLIDA TED BALANCE SHEETS\\n(In millions, except number of shares, which are reflected in thousands, and par value)\\nSeptember 30,\\n2023September 24,\\n2022\\nASSETS:\\nCurrent assets:\\nCash and cash equivalents $ 29,965 $ 23,646 \\nMarketable securities 31,590 24,658 \\nAccounts receivable, net 29,508 28,184 \\nVendor non-trade receivables 31,477 32,748 \\nInventories 6,331 4,946 \\nOther current assets 14,695 21,223 \\nTotal current assets 143,566 135,405 \\nNon-current assets:\\nMarketable securities 100,544 120,805 \\nProperty , plant and equipment, net 43,715 42,117 \\nOther non-current assets 64,758 54,428 \\nTotal non-current assets 209,017 217,350 \\nTotal assets $ 352,583 $ 352,755 \\nLIABILITIES AND SHAREHOLDERS’  EQUITY :\\nCurrent liabilities:', 'Other income/(expense), net (565) (334) 258 \\nIncome before provision for income taxes 113,736 119,103 109,207 \\nProvision for income taxes 16,741 19,300 14,527 \\nNet income $ 96,995 $ 99,803 $ 94,680 \\nEarnings per share:\\nBasic $ 6.16 $ 6.15 $ 5.67 \\nDiluted $ 6.13 $ 6.11 $ 5.61 \\nShares used in computing earnings per share:\\nBasic 15,744,231 16,215,963 16,701,272 \\nDiluted 15,812,547 16,325,819 16,864,919 \\nSee accompanying Notes to Consolidated Financial Statements.\\nApple Inc. | 2023 Form 10-K | 28']\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "3 validation errors for SystemMessage\ncontent\n  str type expected (type=type_error.str)\ncontent -> 1\n  str type expected (type=type_error.str)\ncontent -> 1\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ask a question ; uncomment to test\u001b[39;00m\n\u001b[1;32m      2\u001b[0m user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide your question here :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqueryAIModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_question\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 99\u001b[0m, in \u001b[0;36mqueryAIModel\u001b[0;34m(user_input, llm_name, use_hyde)\u001b[0m\n\u001b[1;32m     96\u001b[0m  system_prompt \u001b[38;5;241m=\u001b[39m build_system_prompt(user_input)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# print(system_prompt)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m  messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 99\u001b[0m      \u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m     \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    102\u001b[0m      HumanMessage(\n\u001b[1;32m    103\u001b[0m          content\u001b[38;5;241m=\u001b[39muser_input\n\u001b[1;32m    104\u001b[0m      ),\n\u001b[1;32m    105\u001b[0m  ]\n\u001b[1;32m    106\u001b[0m  Print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m  output \u001b[38;5;241m=\u001b[39m conversationChat\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/messages/base.py:45\u001b[0m, in \u001b[0;36mBaseMessage.__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, List[Union[\u001b[38;5;28mstr\u001b[39m, Dict]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     43\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 3 validation errors for SystemMessage\ncontent\n  str type expected (type=type_error.str)\ncontent -> 1\n  str type expected (type=type_error.str)\ncontent -> 1\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "# Ask a question ; uncomment to test\n",
    "user_question = input(\"Please provide your question here :\")\n",
    "result = queryAIModel(user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I can't provide the information you're looking for. As an AI, I don't have real-time data or future predictions. My training data only includes information up to September 2021, and I don't have the ability to access or retrieve personal data unless it has been shared with me in the course of our conversation. I'm designed to respect user privacy and confidentiality.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.00419,\n",
       " 'namespaces': {'': {'vector_count': 419}},\n",
       " 'total_vector_count': 419}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
