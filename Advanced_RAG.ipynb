{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.chains import ConversationChain, HypotheticalDocumentEmbedder, LLMChain, RetrievalQA\n",
    "from domino_data.vectordb import DominoPineconeConfiguration\n",
    "#from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain import PromptTemplate\n",
    "from langchain_community.chat_models import ChatMlflow\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "#from langchain_community.embeddings import CohereEmbeddings\n",
    "from langchain_community.embeddings import MlflowEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever, CohereRagRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "import sys\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "client = get_deploy_client(os.environ['DOMINO_MLFLOW_DEPLOYMENTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize embedding\n",
    "model = 'text-embedding-ada-002'\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') \n",
    "# The OpenAIEmbeddings class is instantiated with two parameters: \n",
    "# 'model' and 'openai_api_key'. 'model' is the name of the model to be used \n",
    "# and 'openai_api_key' is the key for accessing the OpenAI API.\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=model,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = MlflowEmbeddings(\n",
    "    target_uri=os.environ[\"DOMINO_MLFLOW_DEPLOYMENTS\"],\n",
    "    endpoint=\"embedding-ada-002ja2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PINECONE_ENV=os.getenv('PINECONE_ENV')\n",
    "COHERE_API_KEY=os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cohere_embeddings = CohereEmbeddings(cohere_api_key=COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Domino Pinecone Data Source name \n",
    "datasource_name = \"mrag-fin-docs-ja\"\n",
    "\n",
    "# Load Domino Pinecone Data Source Configuration \n",
    "conf = DominoPineconeConfiguration(datasource=datasource_name)\n",
    "\n",
    "# api_key variable is a mandatory non-empty field used for \n",
    "# the native pinecone python client initialization \n",
    "# using Pinecone Data Source name \n",
    "api_key = os.environ.get(\"DOMINO_VECTOR_DB_METADATA\", datasource_name)\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=api_key, \n",
    "    environment=PINECONE_ENV,\n",
    "    openapi_config=conf # Domino Pinecone Data Source Configuration \n",
    ")\n",
    "\n",
    "# Load Pinecone Index\n",
    "index_name = \"mrag-fin-docs\"\n",
    "index = pinecone.Index(index_name)\n",
    "text_field = \"text\"\n",
    "#vectorstore = Pinecone.from_existing_index(index_name, embeddings.embed_query)\n",
    "# switch back to normal index for langchain\n",
    "vectorstore = Pinecone(\n",
    "    index, embeddings, text_field # Using embedded data from Domino AI Gateway Endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.00361,\n",
       " 'namespaces': {'': {'vector_count': 361}},\n",
       " 'total_vector_count': 361}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseCohere.rerank() takes 1 positional argument but 4 positional arguments (and 2 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m cohere_rerank \u001b[38;5;241m=\u001b[39m CohereRerank()\n\u001b[1;32m      3\u001b[0m compression_retriever \u001b[38;5;241m=\u001b[39m ContextualCompressionRetriever(\n\u001b[1;32m      4\u001b[0m     base_compressor\u001b[38;5;241m=\u001b[39mcohere_rerank, \n\u001b[1;32m      5\u001b[0m     base_retriever\u001b[38;5;241m=\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m})\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[43mcompression_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow many sales?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py:245\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    244\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    248\u001b[0m         result,\n\u001b[1;32m    249\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py:238\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 238\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/retrievers/contextual_compression.py:48\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(\n\u001b[1;32m     45\u001b[0m     query, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[0;32m---> 48\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_cohere/rerank.py:99\u001b[0m, in \u001b[0;36mCohereRerank.compress_documents\u001b[0;34m(self, documents, query, callbacks)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mCompress documents using Cohere's rerank API.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    A sequence of compressed documents.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m compressed \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    100\u001b[0m     doc \u001b[38;5;241m=\u001b[39m documents[res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    101\u001b[0m     doc_copy \u001b[38;5;241m=\u001b[39m Document(doc\u001b[38;5;241m.\u001b[39mpage_content, metadata\u001b[38;5;241m=\u001b[39mdeepcopy(doc\u001b[38;5;241m.\u001b[39mmetadata))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_cohere/rerank.py:71\u001b[0m, in \u001b[0;36mCohereRerank.rerank\u001b[0;34m(self, documents, query, model, top_n, max_chunks_per_doc)\u001b[0m\n\u001b[1;32m     69\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m     70\u001b[0m top_n \u001b[38;5;241m=\u001b[39m top_n \u001b[38;5;28;01mif\u001b[39;00m (top_n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m top_n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_n\n\u001b[0;32m---> 71\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chunks_per_doc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_chunks_per_doc\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m result_dicts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseCohere.rerank() takes 1 positional argument but 4 positional arguments (and 2 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "# Create Cohere's reranker with the vector DB using Cohere's embeddings as the base retriever\n",
    "cohere_rerank = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=cohere_rerank, \n",
    "    base_retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\"How many sales?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatLLM = ChatMlflow(\n",
    "        target_uri=os.environ[\"DOMINO_MLFLOW_DEPLOYMENTS\"],\n",
    "        endpoint=\"chat-gpt4-ja\", \n",
    "        temperature=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"You are an AI assistant with expertise in financial analysis, your task is to dissect corporate filings (e.g., 10-K, 10-Q, 8-K reports) of publicly traded companies and provide detailed, accurate responses to the following user question about the company's financial health, market position, and future prospects. \n",
    "When interacting, adhere to the following guidelines:\n",
    "You are given the following extracted parts of a long document and a question. \n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "If the question is not related to corporate filings analysis, politely inform them that you are tuned to only answer questions pertaining to corporate filings analysis.\n",
    "Your primary objective is to deliver insightful, accurate, and helpful answers that enable users to make informed decisions based on corporate filings analysis. Each response should be tailored to the user's question, enhancing understanding of the company's financial status and strategic direction. pertaining to policy coverage.\n",
    "Question: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Answer in Markdown:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"question\", \"context\"])\n",
    "#\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conversationChat = ConversationChain(\n",
    "    llm=chatLLM,\n",
    "    \n",
    "    memory=ConversationSummaryMemory(llm=chatLLM),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"How can I help you today?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=chatLLM,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                       retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                       return_source_documents=True\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please ask your question: What was the Companyâ€™s total net sales increase during the first quarter of FY2024 \n"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Please ask your question:\")\n",
    "result = qa_chain(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The company's total net sales increased by 2% or $2.4 billion during the first quarter of 2024 compared to the same quarter in 2023. This increase was primarily driven by higher net sales of iPhone and Services, although it was partially offset by lower net sales of iPad and Wearables, Home and Accessories. The company also repurchased $20.5 billion of its common stock and paid dividends and dividend equivalents of $3.8 billion during the first quarter of 2024. \\n\\nHowever, it's important to note that the company's financial performance is subject to risks associated with changes in the value of the U.S. dollar relative to local currencies. Macroeconomic conditions, including inflation, changes in interest rates, and currency fluctuations, have directly and indirectly impacted, and could in the future materially impact, the company's results of operations and financial condition. \\n\\nIn terms of regional performance, net sales increased in Americas, Europe, Japan, and the rest of Asia Pacific by 2%, 10%, 15%, and 7% respectively. However, in Greater China, net sales decreased by 13%. \\n\\nThe company has historically experienced higher net sales in its first quarter compared to other quarters in its fiscal year due in part to seasonal holiday demand. Additionally, new product and service introductions can significantly impact net sales, cost of sales, and operating expenses. The company generates a significant portion of its net sales from a single product and a decline in demand for that product could significantly impact quarterly net sales.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please ask your financial question: Were there any product announcements in q1 of 2024?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compression_retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ask your financial question:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[43mcompression_retriever\u001b[49m\u001b[38;5;241m.\u001b[39mget_relevant_documents(user_question)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Print the relevant documents from using the embeddings and reranker\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(compressed_docs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compression_retriever' is not defined"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Please ask your financial question:\")\n",
    "compressed_docs = compression_retriever.get_relevant_documents(user_question)\n",
    "# Print the relevant documents from using the embeddings and reranker\n",
    "print(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup HyDE\n",
    "\n",
    "hyde_prompt_template = \"\"\"As an advanced Retrieve-and-Generate (RAG) Chatbot with expertise in financial analysis, your task is to dissect corporate filings (e.g., 10-K, 10-Q, 8-K reports) of publicly traded companies and provide detailed, accurate responses to the following user question about the company's financial health, market position, and future prospects. When interacting, adhere to the following guidelines:\n",
    "Your primary objective is to deliver insightful, accurate, and helpful answers that enable users to make informed decisions based on corporate filings analysis. Each response should be tailored to the user's question, enhancing understanding of the company's financial status and strategic direction. \n",
    "\"Please answer the user's question below \\n \n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "hyde_prompt = PromptTemplate(input_variables=[\"question\"], template=hyde_prompt_template)\n",
    "hyde_llm_chain = LLMChain(llm=chatLLM, prompt=hyde_prompt)\n",
    "\n",
    "hyde_embeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=hyde_llm_chain, base_embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get relevant docs through vector DB\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Number of texts to match (may be less if no suitable match)\n",
    "NUM_TEXT_MATCHES = 5\n",
    "\n",
    "# Number of texts to return from reranking\n",
    "NUM_RERANKING_MATCHES = 3\n",
    "\n",
    "# Create prompt\n",
    "template = \"\"\"As an advanced Retrieve-and-Generate (RAG) Chatbot with expertise in financial analysis, your task is to dissect corporate filings (e.g., 10-K, 10-Q, 8-K reports) of publicly traded companies and provide detailed, accurate responses to the following user question about the company's financial health, market position, and future prospects. When interacting, adhere to the following guidelines:If the question is not related to the context of corporate filings, politely respond with, 'Hi, I'm here to help analyze financial documents and related inquiries. Could you ask a question related to the company's corporate filings?'\n",
    "Your primary objective is to deliver insightful, accurate, and helpful answers that enable users to make informed decisions based on corporate filings analysis. Each response should be tailored to the user's question, enhancing understanding of the company's financial status and strategic direction. pertaining to policy coverage.\n",
    "Here is some relevant context: {context}\n",
    "\"\"\"\n",
    "\n",
    "# Load the reranking model\n",
    "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Get relevant docs through vector DB\n",
    "def get_relevant_docs(user_input, num_matches=NUM_TEXT_MATCHES, use_hyde=False):\n",
    "   \n",
    "    if use_hyde:\n",
    "        embedded_query = hyde_embeddings.embed_query(user_input)\n",
    "    else:\n",
    "        embedded_query = embeddings.embed_query(user_input)\n",
    "        \n",
    " \n",
    "    relevant_docs = index.query(\n",
    "        vector=embedded_query,\n",
    "        top_k=num_matches,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    matches = relevant_docs[\"matches\"]\n",
    "    filtered_matches = [match for match in matches if match['score'] >= SIMILARITY_THRESHOLD]\n",
    "    relevant_docs[\"matches\"] = filtered_matches\n",
    "\n",
    "    return relevant_docs\n",
    "\n",
    " \n",
    "def build_system_prompt(user_input, rerank=False, use_hyde=False):\n",
    "    print(user_input)\n",
    "    try:\n",
    "        relevant_docs = get_relevant_docs(user_input)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get relevant documents: {e}\")\n",
    "        return \"\", \"Failed to get relevant documents\"\n",
    "\n",
    "    actual_num_matches = len(relevant_docs[\"matches\"])\n",
    "    if actual_num_matches == 0:\n",
    "        print(\"No matches found in relevant documents.\")\n",
    "        return \"\", \"No matches found in relevant documents\"\n",
    "    \n",
    "    contexts = [relevant_docs[\"matches\"][i][\"metadata\"][\"text\"] for i in range(actual_num_matches)]\n",
    "    print(\"num_matches: \", actual_num_matches)\n",
    "    if rerank and actual_num_matches >= NUM_RERANKING_MATCHES:\n",
    "        try:\n",
    "            docs = colbert.rerank(query=user_input, documents=contexts, k=NUM_RERANKING_MATCHES)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to rerank documents: {e}\")\n",
    "            return \"\", \"Failed to rerank documents\"\n",
    "        \n",
    "        try:\n",
    "            result_indices = [docs[i][\"result_index\"] for i in range(NUM_RERANKING_MATCHES)]\n",
    "        except (IndexError, KeyError) as e:\n",
    "                print(f\"Invalid result indices: {e}\")\n",
    "                return \"\", \"Invalid result indices\"\n",
    "        try:    \n",
    "            contexts = [contexts[index] for index in result_indices]\n",
    "        except IndexError as e:\n",
    "            print(f\"Indexing error: {e}\")\n",
    "            return \"\", \"Indexing error\"\n",
    "    \n",
    "    # Create prompt\n",
    "    template = \"\"\"As an advanced Retrieve-and-Generate (RAG) Chatbot with expertise in financial analysis, your task is to dissect corporate filings (e.g., 10-K, 10-Q, 8-K reports) of publicly traded companies and provide detailed, accurate responses to the following user question about the company's financial health, market position, and future prospects. When interacting, adhere to the following guidelines\n",
    "Your primary objective is to deliver insightful, accurate, and helpful answers that enable users to make informed decisions based on corporate filings analysis. Each response should be tailored to the user's question, enhancing understanding of the company's financial status and strategic direction.\n",
    "Here is some relevant context: {context}\"\"\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    system_prompt = prompt_template.format(context=contexts)\n",
    "    print(contexts)\n",
    "    return system_prompt, contexts\n",
    "\n",
    "# Query the Open AI Model\n",
    "def queryAIModel(user_input, llm_name=\"openai\", use_hyde=False):\n",
    "\n",
    "    if not user_input:\n",
    "        return \"Please provide an input\"\n",
    "    \n",
    "    system_prompt = build_system_prompt(user_input)\n",
    "   # print(system_prompt)\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=system_prompt\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=user_input\n",
    "        ),\n",
    "    ]\n",
    "    Print(\"hi\")\n",
    "    output = conversationChat.predict(input=messages)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ask a question ; uncomment to test\n",
    "user_question = input(\"Please provide your question here :\")\n",
    "result = queryAIModel(user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe_index_stats()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /mnt/code/data/apple-10Q-2023Q4.pdf has been added.\n"
     ]
    }
   ],
   "source": [
    "%run /mnt/code/services/data_ingestion.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
